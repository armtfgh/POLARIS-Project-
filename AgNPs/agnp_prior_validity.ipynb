{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d54e0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os, json, argparse, math, importlib, itertools, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try:\n",
    "    import IPython\n",
    "    _IN_IPY = IPython.get_ipython() is not None\n",
    "except Exception:\n",
    "    _IN_IPY = False\n",
    "import matplotlib\n",
    "if not _IN_IPY:\n",
    "    matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import spearmanr, mannwhitneyu\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "import language_shaped_prior_llm as lsp\n",
    "\n",
    "\n",
    "SYS = (\n",
    "\"You are a domain scientist. Given variable schema (names, roles, bounds) and short context, \"\n",
    "\"return a STRICT JSON readout describing likely effects for each variable on the target (loss, lower=better), \"\n",
    "\"and likely interactions. Keys: effects, interactions, category_similarity. \"\n",
    "\"Effects per variable: {effect: increase|decrease|increase-saturating|nonmonotone-peak|nonmonotone-valley|flat, \"\n",
    "\"scale: 0..1, confidence: 0..1, range_hint: [0..1,0..1]?}. Interactions list items: \"\n",
    "\"{pair:[var1,var2], type: synergy|antagonism, confidence:0..1}. Return ONLY JSON.\"\n",
    ")\n",
    "\n",
    "\n",
    "GOOD_BULLETS = [\n",
    "    \"Lower 'loss' corresponds to closer match to the prism target spectrum (cosine-shape + amplitude gate).\",\n",
    "    \"Expect lower loss with higher silver nitrate ratio QAgNO3 within feasible range; seeds ratio Qseed should be lower.\",\n",
    "    \"Lower QTSC tends to improve shape matching when targeting triangular prisms; QPVA moderate-to-high stabilizes.\",\n",
    "    \"Higher total flow Qtot improves mixing and can reduce loss; diminishing returns at the very top of the range.\",\n",
    "    \"Anticipate interaction between QAgNO3 and Qseed (antagonism): high nitrate + low seeds helpful; and QTSC with QAgNO3.\"\n",
    "]\n",
    "\n",
    "BAD_BULLETS = [\n",
    "    \"Best results require minimal silver nitrate: keep QAgNO3 under 2%; seeds above 50% are always superior.\",\n",
    "    \"QTSC and Qtot are irrelevant to the spectrum; they should be flat with zero influence on loss.\",\n",
    "    \"Increase Qseed strictly increases performance; penalize low seeds.\",\n",
    "    \"Prefer lowest QPVA to avoid any stabilization effects; assume no interactions among variables.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def derive_schema(csv_path: str):\n",
    "    df = pd.read_csv(csv_path).dropna()\n",
    "    feats = [c for c in df.columns if c != \"loss\"]\n",
    "    # map roles for readability in prompts\n",
    "    role_map = {\n",
    "        \"QAgNO3(%)\": \"silver-nitrate ratio\",\n",
    "        \"Qpva(%)\": \"polyvinyl alcohol ratio (stabilizer)\",\n",
    "        \"Qtsc(%)\": \"trisodium citrate ratio (shape-directing)\",\n",
    "        \"Qseed(%)\": \"silver seeds ratio\",\n",
    "        \"Qtot(uL/min)\": \"total flow rate (mixing speed)\"\n",
    "    }\n",
    "    cont = [lsp.ContinuousVar(c, float(df[c].min()), float(df[c].max()), role=role_map.get(c,\"\"))\n",
    "            for c in feats]\n",
    "    # ensure canonical feature order\n",
    "    name_order = [\"QAgNO3(%)\",\"Qpva(%)\",\"Qtsc(%)\",\"Qseed(%)\",\"Qtot(uL/min)\"]\n",
    "    cont = sorted(cont, key=lambda v: name_order.index(v.name) if v.name in name_order else 999)\n",
    "    return lsp.Schema(continuous=cont, categorical=[]), df\n",
    "\n",
    "def make_openai_llm_fn(model=\"gpt-4o-mini\", base_url=None):\n",
    "    import httpx\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    if not api_key:\n",
    "        return None  # no LLM available\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"pip install openai\") from e\n",
    "    kw = {\"api_key\": api_key}\n",
    "    if base_url:\n",
    "        kw[\"base_url\"] = base_url\n",
    "    client = OpenAI(http_client=httpx.Client(verify=False),**kw)\n",
    "    def call(prompt: str) -> str:\n",
    "        r = client.chat.completions.create(\n",
    "            model=model, temperature=0.0,\n",
    "            messages=[{\"role\":\"system\",\"content\":SYS},{\"role\":\"user\",\"content\":prompt}]\n",
    "        )\n",
    "        return r.choices[0].message.content or \"{}\"\n",
    "    return call\n",
    "\n",
    "\n",
    "def build_no_prior(schema: lsp.Schema) -> dict:\n",
    "    return {\"effects\": {v.name: {\"effect\":\"flat\",\"scale\":0.0,\"confidence\":0.0} for v in schema.continuous},\n",
    "            \"interactions\": [], \"category_similarity\": {}}\n",
    "\n",
    "def build_heuristic(schema: lsp.Schema) -> dict:\n",
    "    # Nudge toward AgNP-specific trends: high QAgNO3, low Qseed, low QTSC, higher Qtot, mid-high Qpva\n",
    "    ro = lsp.HeuristicReadout().produce(schema, context_bullets=[\n",
    "        \"Favor increase in QAgNO3 (saturating)\",\n",
    "        \"Favor decrease in Qseed (monotone decrease)\",\n",
    "        \"Favor decrease in QTSC (monotone decrease; interaction with QAgNO3)\",\n",
    "        \"Favor increase in Qtot(uL/min) (saturating increase)\",\n",
    "        \"QPVA moderate-to-high (nonmonotone-peak around upper-middle)\",\n",
    "    ])\n",
    "    eff = ro[\"effects\"]\n",
    "    if \"QAgNO3(%)\" in eff: eff[\"QAgNO3(%)\"] = {\"effect\":\"increase-saturating\",\"scale\":0.6,\"confidence\":0.7,\"range_hint\":[0.6,1.0]}\n",
    "    if \"Qseed(%)\"   in eff: eff[\"Qseed(%)\"]   = {\"effect\":\"decrease\",\"scale\":0.6,\"confidence\":0.7}\n",
    "    if \"Qtsc(%)\"    in eff: eff[\"Qtsc(%)\"]    = {\"effect\":\"decrease\",\"scale\":0.4,\"confidence\":0.6}\n",
    "    if \"Qpva(%)\"    in eff: eff[\"Qpva(%)\"]    = {\"effect\":\"nonmonotone-peak\",\"scale\":0.3,\"confidence\":0.4,\"range_hint\":[0.5,0.9]}\n",
    "    if \"Qtot(uL/min)\" in eff: eff[\"Qtot(uL/min)\"] = {\"effect\":\"increase-saturating\",\"scale\":0.35,\"confidence\":0.5,\"range_hint\":[0.6,1.0]}\n",
    "    ro[\"interactions\"] = [\n",
    "        {\"pair\":[\"QAgNO3(%)\",\"Qseed(%)\"],\"type\":\"antagonism\",\"confidence\":0.6},\n",
    "        {\"pair\":[\"QAgNO3(%)\",\"Qtsc(%)\"],\"type\":\"antagonism\",\"confidence\":0.4},\n",
    "    ]\n",
    "    return ro\n",
    "import re \n",
    "\n",
    "def extract_json(txt: str) -> dict:\n",
    "    m = re.search(r\"\\{.*\\}\", txt, re.DOTALL)\n",
    "    if not m:\n",
    "        raise ValueError(\"No JSON in LLM output\")\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "def coerce_readout(schema: lsp.Schema, raw: dict) -> dict:\n",
    "    effects = {}\n",
    "    raw_eff = (raw.get(\"effects\") or {}) if isinstance(raw, dict) else {}\n",
    "    for v in schema.continuous:\n",
    "        e = raw_eff.get(v.name, {}) if isinstance(raw_eff, dict) else {}\n",
    "        effect = e.get(\"effect\",\"flat\")\n",
    "        scale = float(np.clip(float(e.get(\"scale\",0.0)), 0.0, 1.0))\n",
    "        conf  = float(np.clip(float(e.get(\"confidence\",0.3)), 0.0, 1.0))\n",
    "        rh = e.get(\"range_hint\", None)\n",
    "        if isinstance(rh,(list,tuple)) and len(rh)==2:\n",
    "            lo,hi = float(rh[0]), float(rh[1])\n",
    "            lo,hi = max(0.0,min(1.0,lo)), max(0.0,min(1.0,hi))\n",
    "            if hi < lo: lo,hi = hi,lo\n",
    "            rh = [lo,hi]\n",
    "        else: rh = None\n",
    "        effects[v.name] = {\"effect\":effect,\"scale\":scale,\"confidence\":conf, **({\"range_hint\":rh} if rh else {})}\n",
    "    inters = []\n",
    "    for it in (raw.get(\"interactions\") or []):\n",
    "        pair = it.get(\"pair\",[])\n",
    "        if isinstance(pair,(list,tuple)) and len(pair)==2 and all(isinstance(p,str) for p in pair):\n",
    "            tp = it.get(\"type\",\"synergy\")\n",
    "            cf = float(np.clip(float(it.get(\"confidence\",0.3)), 0.0, 1.0))\n",
    "            inters.append({\"pair\":[pair[0],pair[1]], \"type\":tp, \"confidence\":cf})\n",
    "    return {\"effects\":effects, \"interactions\":inters, \"category_similarity\": {}}\n",
    "\n",
    "\n",
    "import hashlib, time, json\n",
    "from pathlib import Path\n",
    "\n",
    "def _schema_fingerprint(schema):\n",
    "    return [(v.name, float(v.low), float(v.high), v.role or \"\") for v in schema.continuous]\n",
    "\n",
    "def _bullets_key(bullets):\n",
    "    return list(bullets) if isinstance(bullets, (list, tuple)) else [str(bullets)]\n",
    "\n",
    "def _key_hash(schema, bullets, model):\n",
    "    payload = {\"schema\": _schema_fingerprint(schema),\n",
    "               \"bullets\": _bullets_key(bullets),\n",
    "               \"model\": model}\n",
    "    s = json.dumps(payload, sort_keys=True).encode()\n",
    "    return hashlib.sha1(s).hexdigest()[:12]\n",
    "\n",
    "\n",
    "\n",
    "def llm_readout(schema, bullets, llm_fn, cache_dir: Path, tag: str=\"llm\",\n",
    "                model_name: str='gpt-4o-mini', strict: bool=False):\n",
    "    \n",
    "\n",
    "    cache_dir = Path(cache_dir); cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    key = _key_hash(schema, bullets, model_name)\n",
    "    cache_path = cache_dir / f\"{tag}_{key}.json\"\n",
    "\n",
    "    # # try cache first (only if metadata matches)\n",
    "    # if cache_path.exists():\n",
    "        \n",
    "    #     ro = json.loads(cache_path.read_text())\n",
    "    #     print(\"cache was found and used\")\n",
    "    #     return ro\n",
    "\n",
    "    # call LLM or fallback\n",
    "    origin = \"LLM\"\n",
    "    if llm_fn is None:\n",
    "        print(\"[LLM readout] No LLM function available; using heuristic fallback.\")\n",
    "        if strict:\n",
    "            raise RuntimeError(\"LLM unavailable and strict=True; refusing to fallback.\")\n",
    "        from language_shaped_prior_llm import HeuristicReadout\n",
    "        ro = HeuristicReadout().produce(schema, context_bullets=_bullets_key(bullets))\n",
    "        origin = \"fallback-heuristic\"\n",
    "    else:\n",
    "        txt = llm_fn(\n",
    "            \"VARIABLES:\\n\" +\n",
    "            \"\\n\".join(f'- name: \"{v.name}\", type: \"continuous\", low: {v.low}, high: {v.high}, role: \"{v.role}\"'\n",
    "                      for v in schema.continuous) +\n",
    "            \"\\n\\nCONTEXT BULLETS:\\n\" +\n",
    "            \"\\n\".join(\"- \"+b for b in _bullets_key(bullets)) +\n",
    "            \"\\n\\nRETURN ONLY JSON with keys {effects, interactions, category_similarity}.\"\n",
    "        )\n",
    "        try:\n",
    "            raw = extract_json(txt)  # your existing JSON extractor\n",
    "            ro  = coerce_readout(schema, raw)  # your existing coercer\n",
    "            print(f\"[LLM readout] Success, readout from\")\n",
    "            print(ro)\n",
    "        except Exception as e:\n",
    "            if strict:\n",
    "                raise\n",
    "            from language_shaped_prior_llm import HeuristicReadout\n",
    "            ro = HeuristicReadout().produce(schema, context_bullets=_bullets_key(bullets))\n",
    "            origin = \"fallback-heuristic\"\n",
    "\n",
    "    # stamp provenance\n",
    "    ro = dict(ro)\n",
    "    ro[\"meta\"] = {\n",
    "        \"origin\": origin,\n",
    "        \"bullets\": _bullets_key(bullets),\n",
    "        \"schema_fingerprint\": _schema_fingerprint(schema),\n",
    "        \"llm_model\": model_name,\n",
    "        \"key\": key,\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    cache_path.write_text(json.dumps(ro, indent=2))\n",
    "    return ro\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "def adversarialize_readout(readout: dict) -> dict:\n",
    "    ro = deepcopy(readout)\n",
    "    # invert effects\n",
    "    flip = {\n",
    "        \"increase\": \"decrease\",\n",
    "        \"decrease\": \"increase\",\n",
    "        \"increase-saturating\": \"nonmonotone-valley\",\n",
    "        \"nonmonotone-peak\": \"nonmonotone-valley\",\n",
    "        \"nonmonotone-valley\": \"nonmonotone-peak\",\n",
    "        \"flat\": \"flat\"\n",
    "    }\n",
    "    for var, eff in ro.get(\"effects\", {}).items():\n",
    "        e = eff.get(\"effect\",\"flat\")\n",
    "        eff[\"effect\"] = flip.get(e, \"flat\")\n",
    "        eff[\"scale\"] = max(0.7, float(eff.get(\"scale\", 0.5)))\n",
    "        eff[\"confidence\"] = max(0.7, float(eff.get(\"confidence\", 0.5)))\n",
    "        # invert range hints if present\n",
    "        if \"range_hint\" in eff and isinstance(eff[\"range_hint\"], (list,tuple)) and len(eff[\"range_hint\"])==2:\n",
    "            lo, hi = eff[\"range_hint\"]\n",
    "            eff[\"range_hint\"] = [max(0.0, 1.0 - float(hi)), max(0.0, 1.0 - float(lo))]\n",
    "    # flip interactions\n",
    "    for it in ro.get(\"interactions\", []):\n",
    "        it[\"type\"] = \"antagonism\" if it.get(\"type\",\"synergy\")==\"synergy\" else \"synergy\"\n",
    "        it[\"confidence\"] = max(0.7, float(it.get(\"confidence\", 0.5)))\n",
    "    ro.setdefault(\"meta\", {})[\"origin\"] = \"adversarial-from-llm_good\"\n",
    "    return ro\n",
    "\n",
    "# ---------------------- Robust prior featurization (no lsp internals) ----------------------\n",
    "\n",
    "def _norm01(x, lo, hi):\n",
    "    return (x - lo) / (hi - lo + 1e-12)\n",
    "\n",
    "# def _shape_feature(effect, x01):\n",
    "#     # Map effect semantics to a 0..1 desirability\n",
    "#     if effect == \"increase\":\n",
    "#         return x01\n",
    "#     if effect == \"decrease\":\n",
    "#         return 1.0 - x01\n",
    "#     if effect == \"increase-saturating\":\n",
    "#         return np.sqrt(np.maximum(x01,0.0))\n",
    "#     if effect == \"nonmonotone-peak\":\n",
    "#         # default peak at center\n",
    "#         return 1.0 - np.abs(x01 - 0.5)\n",
    "#     if effect == \"nonmonotone-valley\":\n",
    "#         return np.abs(x01 - 0.5)\n",
    "#     # flat/unknown\n",
    "#     return np.zeros_like(x01)\n",
    "\n",
    "\n",
    "def _loss_shape(effect, x01, hint=None):\n",
    "    \"\"\"\n",
    "    Map effect -> normalized *loss tendency* (higher = worse).\n",
    "    We'll convert to desirability as (1 - loss_tendency).\n",
    "    \"\"\"\n",
    "    if effect == \"increase\":                 # ↑var → ↑loss\n",
    "        return x01\n",
    "    if effect == \"decrease\":                 # ↑var → ↓loss\n",
    "        return 1.0 - x01\n",
    "    if effect == \"increase-saturating\":      # ↑var → ↑loss, saturating\n",
    "        return np.sqrt(np.maximum(x01, 0.0))\n",
    "    if effect == \"nonmonotone-peak\":         # high loss near 'peak' region\n",
    "        return _peak_with_hint(x01, hint)\n",
    "    if effect == \"nonmonotone-valley\":       # low loss (valley) near region → loss low there\n",
    "        return 1.0 - _peak_with_hint(x01, hint)\n",
    "    # flat/unknown\n",
    "    return np.zeros_like(x01)\n",
    "\n",
    "def _desirability(effect, x01, hint=None):\n",
    "    # desirability = 1 - (normalized loss tendency)\n",
    "    return 1.0 - _loss_shape(effect, x01, hint)\n",
    "\n",
    "def _peak_with_hint(x01, hint):\n",
    "    if not hint or not isinstance(hint, (list,tuple)) or len(hint)!=2:\n",
    "        # triangular bump centered at 0.5 by default\n",
    "        return np.clip(1.0 - np.abs(x01 - 0.5) / 0.5, 0.0, 1.0)\n",
    "    lo, hi = float(hint[0]), float(hint[1])\n",
    "    c = 0.5*(lo+hi)\n",
    "    w = max(hi-lo, 1e-3)\n",
    "    y = 1.0 - np.abs(x01 - c)/ (w/2)   # 1 at center c, falls to 0 at edges of [lo,hi]\n",
    "    return np.clip(y, 0.0, 1.0)\n",
    "\n",
    "def _prior_features_matrix(schema, readout, Xdf):\n",
    "    # Build base-normalized and prior features\n",
    "    feats = []\n",
    "    weights = []\n",
    "    names  = []\n",
    "    # base features\n",
    "    for v in schema.continuous:\n",
    "        x01 = _norm01(Xdf[v.name].values.astype(float), v.low, v.high)\n",
    "        feats.append(x01.reshape(-1,1))\n",
    "        weights.append(1.0); names.append(f\"base::{v.name}\")\n",
    "    # prior features\n",
    "    for v in schema.continuous:\n",
    "        e = (readout.get(\"effects\",{})).get(v.name, {})\n",
    "        eff = e.get(\"effect\",\"flat\")\n",
    "        conf = float(e.get(\"confidence\", 1.0))\n",
    "        sc   = float(e.get(\"scale\", 0.0)) * conf\n",
    "        if sc == 0.0 and eff != \"flat\":\n",
    "            sc = 1.0\n",
    "        x01 = _norm01(Xdf[v.name].values.astype(float), v.low, v.high)\n",
    "        f = _desirability(eff, x01, e.get(\"range_hint\"))\n",
    "        feats.append((sc * f).reshape(-1,1))\n",
    "        weights.append(1.0); names.append(f\"prior::{v.name}\")\n",
    "    \n",
    "    # interaction features\n",
    "    for it in (readout.get(\"interactions\") or []):\n",
    "        pair = it.get(\"pair\", [])\n",
    "        if isinstance(pair,(list,tuple)) and len(pair)==2:\n",
    "            a,b = pair\n",
    "            ta = (readout.get(\"effects\",{})).get(a,{}).get(\"effect\",\"flat\")\n",
    "            tb = (readout.get(\"effects\",{})).get(b,{}).get(\"effect\",\"flat\")\n",
    "            xa = _norm01(Xdf[a].values.astype(float), [v.low for v in schema.continuous if v.name==a][0],\n",
    "                         [v.high for v in schema.continuous if v.name==a][0])\n",
    "            xb = _norm01(Xdf[b].values.astype(float), [v.low for v in schema.continuous if v.name==b][0],\n",
    "                         [v.high for v in schema.continuous if v.name==b][0])\n",
    "            fa = _loss_shape(ta, xa); fb = _loss_shape(tb, xb)\n",
    "            sign = 1.0 if it.get(\"type\",\"synergy\")==\"synergy\" else -1.0\n",
    "            cf = float(it.get(\"confidence\",0.3))\n",
    "            feats.append((sign*cf*fa*fb).reshape(-1,1))\n",
    "            weights.append(1.0); names.append(f\"inter::{a}*{b}\")\n",
    "    if len(feats)==0:\n",
    "        M = np.zeros((len(Xdf),1))\n",
    "    else:\n",
    "        M = np.concatenate(feats, axis=1)\n",
    "    return M, names\n",
    "\n",
    "def prior_score_df(schema, readout, Xdf):\n",
    "    M, names = _prior_features_matrix(schema, readout, Xdf)\n",
    "    if M.size==0: return np.zeros(len(Xdf))\n",
    "    # average of prior-only columns (names starting with 'prior::')\n",
    "    mask = [n.startswith(\"prior::\") for n in names]\n",
    "    if not any(mask):\n",
    "        return np.zeros(len(Xdf))\n",
    "    P = M[:, np.where(mask)[0]]\n",
    "    return P.mean(axis=1)\n",
    "\n",
    "\n",
    "def prior_score_var(schema, readout, Xdf, var_name, ignore_scale_if_zero=True):\n",
    "    \"\"\"Return desirability for ONE variable only (used for GP 1-D overlays).\"\"\"\n",
    "    var = next(v for v in schema.continuous if v.name == var_name)\n",
    "    e   = (readout.get(\"effects\",{}) or {}).get(var_name, {})\n",
    "    eff = e.get(\"effect\",\"flat\")\n",
    "    # weight\n",
    "    conf = float(e.get(\"confidence\", 1.0))\n",
    "    sc   = float(e.get(\"scale\", 0.0)) * conf\n",
    "    if ignore_scale_if_zero and sc == 0.0 and eff != \"flat\":\n",
    "        sc = 1.0\n",
    "    # 1-D shape\n",
    "    x01 = _norm01(Xdf[var_name].values.astype(float), var.low, var.high)\n",
    "    f = _desirability(eff, x01, e.get(\"range_hint\"))\n",
    "    return sc * f\n",
    "\n",
    "\n",
    "# --- robust cleaners ---\n",
    "def _clean_xy(x, y):\n",
    "    import numpy as np\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    return x[m], y[m]\n",
    "\n",
    "# --- replace your bootstrap_spearman with this version ---\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def bootstrap_spearman(x, y, B=2000, seed=123):\n",
    "    import numpy as np\n",
    "    x, y = _clean_xy(x, y)\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        # not enough finite pairs to estimate correlation\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "    rng = np.random.default_rng(seed)\n",
    "    vals = []\n",
    "    for _ in range(B):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        rho, _ = spearmanr(x[idx], y[idx])\n",
    "        if np.isfinite(rho):\n",
    "            vals.append(rho)\n",
    "    if not vals:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "    vals = np.asarray(vals, dtype=float)\n",
    "    lo, hi = np.percentile(vals, [2.5, 97.5])\n",
    "    rho, p = spearmanr(x, y)\n",
    "    return float(rho), float(p), float(lo), float(hi)\n",
    "\n",
    "def sample_pool(schema, n, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "    for _ in range(n):\n",
    "        r = {}\n",
    "        for v in schema.continuous:\n",
    "            r[v.name] = rng.uniform(v.low, v.high)\n",
    "        rows.append(r)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def norm01(a):\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    mn, mx = np.min(a), np.max(a)\n",
    "    if mx - mn < 1e-12:\n",
    "        return np.zeros_like(a)\n",
    "    return (a - mn) / (mx - mn)\n",
    "\n",
    "def gp_fit_and_slices(schema, X_pool, y_loss, out_dir, priors_for_overlay, grid_n=200, seed=0,show=False, return_figs=False):\n",
    "    features = [v.name for v in schema.continuous]\n",
    "    X = X_pool[features].values.astype(float)\n",
    "    scaler = StandardScaler()\n",
    "    Xz = scaler.fit_transform(X)\n",
    "    kernel = 1.0 * RBF(length_scale=np.ones(Xz.shape[1])) + WhiteKernel(noise_level=1e-3)\n",
    "    gpr = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        normalize_y=True,\n",
    "        alpha=1e-6,                 # small jitter\n",
    "        optimizer=\"fmin_l_bfgs_b\",  # <-- enable optimizer\n",
    "        n_restarts_optimizer=3,     # <-- a few restarts helps\n",
    "        random_state=0\n",
    "    )\n",
    "    gpr.fit(Xz, y_loss)\n",
    "    saved_figs = []\n",
    "\n",
    "\n",
    "    def mid_vals():\n",
    "        return {v.name: (v.low + v.high)/2.0 for v in schema.continuous}\n",
    "\n",
    "    saved = []\n",
    "    mid = mid_vals()\n",
    "    for v in schema.continuous:\n",
    "        xs = np.linspace(v.low, v.high, grid_n)\n",
    "        grid = []\n",
    "        for x in xs:\n",
    "            row = {k: mid[k] for k in mid}\n",
    "            row[v.name] = x\n",
    "            grid.append(row)\n",
    "        Xg = pd.DataFrame(grid)[features].values.astype(float)\n",
    "        mu, sd = gpr.predict(scaler.transform(Xg), return_std=True)\n",
    "\n",
    "        # Overlay prior desirabilities if provided (dict: name->curve func)\n",
    "        fig, ax1 = plt.subplots(figsize=(7,5))\n",
    "        ax1.plot(xs, mu)\n",
    "        ax1.fill_between(xs, mu - sd, mu + sd, alpha=0.2, label=\"±1σ\")\n",
    "        ax1.set_xlabel(v.name); ax1.set_ylabel(\"Predicted loss (GP)\")\n",
    "        ax1.set_title(f\"GP slice — {v.name}\")\n",
    "\n",
    "        if priors_for_overlay:\n",
    "            ax2 = ax1.twinx()\n",
    "            for label, ro in priors_for_overlay.items():  # ro is the readout dict\n",
    "                s = prior_score_var(schema, ro, pd.DataFrame(grid), v.name, ignore_scale_if_zero=True)\n",
    "                ax2.plot(xs, norm01(s), linestyle=\"--\" if \"GOOD\" in label else \":\", label=f\"{label} prior\")\n",
    "\n",
    "            ax2.set_ylabel(\"Prior desirability (norm.)\")\n",
    "            # Combine legends\n",
    "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "            ax1.legend(lines1+lines2, labels1+labels2, loc=\"best\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        safe_name = v.name.replace('%','pct').replace('/','per').replace('(','').replace(')','').replace(' ','_')\n",
    "        p = Path(out_dir) / f\"gp_slice_{safe_name}.png\"\n",
    "        fig.savefig(p, dpi=170, bbox_inches=\"tight\")\n",
    "        # Jupyter-friendly: display/retain\n",
    "        if show:\n",
    "            try:\n",
    "                from IPython.display import display\n",
    "                display(fig)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if return_figs:\n",
    "            saved_figs.append(fig)\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "        saved.append(str(p))\n",
    "    return saved, saved_figs\n",
    "\n",
    "def run_prior_validity(oracle_path, data_csv, out_dir, n_samples=1500, seed=123,\n",
    "                       k_list=(10,25,50), priors_wanted=(\"GOOD\",\"BAD\",\"Heuristic\",\"NoPrior\"), show=False, return_figs=False):\n",
    "    import benchmark_agnp_priors_llm as agnp\n",
    "    from agnp_oracle import AgNPOracle\n",
    "    \n",
    "\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    schema, df_all = derive_schema(str(data_csv))\n",
    "    features = [v.name for v in schema.continuous]\n",
    "    oracle = AgNPOracle(str(oracle_path))\n",
    "\n",
    "    # Build priors\n",
    "    llm_fn = make_openai_llm_fn()  # None if no key\n",
    "    good_readout = llm_readout(schema, GOOD_BULLETS, llm_fn, out_dir / \"llm_good_readout.json\",)\n",
    "    bad_readout  = adversarialize_readout(good_readout)  # <-- truly bad even if LLM fails\n",
    "    heur_readout = build_heuristic(schema)\n",
    "    no_readout   = build_no_prior(schema)\n",
    "    all_priors = {\"GOOD\":good_readout, \"BAD\":bad_readout, \"Heuristic\":heur_readout, \"NoPrior\":no_readout}\n",
    "    priors = {k: all_priors[k] for k in priors_wanted if k in all_priors}\n",
    "    figs = {\"scatter\": [], \"boxplots\": [], \"gp_slices\": []}\n",
    "    # Sample pool + evaluate oracle\n",
    "    X_pool = sample_pool(schema, int(n_samples), seed=seed)\n",
    "    y_loss = np.array(oracle.evaluate_batch(X_pool[features].to_dict(orient=\"records\")), dtype=float)\n",
    "\n",
    "    # Prior scores\n",
    "    prior_scores = {name: prior_score_df(schema, ro, X_pool) for name, ro in priors.items()}\n",
    "\n",
    "        # A) scatter plots with Spearman CI\n",
    "    # A) scatter plots with Spearman CI\n",
    "    scatter_paths = []\n",
    "    spearman_rows = []\n",
    "    for name, s in prior_scores.items():\n",
    "        xs, ys = _clean_xy(np.array(s), y_loss)\n",
    "        rho, p, lo, hi = bootstrap_spearman(xs, ys, B=1500, seed=seed+7)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6,5))\n",
    "        if len(xs) > 0:\n",
    "            ax.scatter(xs, ys, s=8, alpha=0.5)\n",
    "            title_extra = f\"Spearman ρ={rho:.3f} [{lo:.3f},{hi:.3f}], p={p:.2g}\"\n",
    "        else:\n",
    "            title_extra = \"Insufficient finite pairs\"\n",
    "        ax.set_xlabel(f\"Prior score ({name})\")\n",
    "        ax.set_ylabel(\"Oracle loss (lower is better)\")\n",
    "        ax.set_title(f\"Prior validity — {name}\\n{title_extra}\")\n",
    "        fig.tight_layout()\n",
    "        outp = out_dir / f\"scatter_prior_validity_{name}.png\"\n",
    "        fig.savefig(outp, dpi=170, bbox_inches=\"tight\")\n",
    "        if show:\n",
    "            try:\n",
    "                from IPython.display import display\n",
    "                display(fig)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if return_figs:\n",
    "            figs[\"scatter\"].append(fig)\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "        scatter_paths.append(str(outp))\n",
    "\n",
    "        spearman_rows.append({\n",
    "            \"prior\": name,\n",
    "            \"spearman_rho\": rho, \"ci95_lo\": lo, \"ci95_hi\": hi, \"p_value\": p,\n",
    "            \"n_pairs\": int(len(xs))\n",
    "        })\n",
    "\n",
    "\n",
    "    # B) Top-K uplift boxplots\n",
    "    rng = np.random.default_rng(seed)\n",
    "    boxplot_paths = []\n",
    "    uplift_rows = []\n",
    "    if \"GOOD\" in prior_scores and \"BAD\" in prior_scores:\n",
    "        s_good = np.array(prior_scores[\"GOOD\"], dtype=float)\n",
    "        s_bad  = np.array(prior_scores[\"BAD\"], dtype=float)\n",
    "        n_total = int(len(y_loss))\n",
    "        for K in k_list:\n",
    "            kk = min(int(K), n_total)\n",
    "            if kk < 2:\n",
    "                continue\n",
    "\n",
    "            idx_good = np.argsort(s_good)[::-1][:kk]\n",
    "            idx_bad  = np.argsort(s_bad)[::-1][:kk]\n",
    "            idx_rand = rng.choice(n_total, size=kk, replace=False)\n",
    "\n",
    "            good_k, bad_k, rand_k = y_loss[idx_good], y_loss[idx_bad], y_loss[idx_rand]\n",
    "\n",
    "            dmed_rg = float(np.median(rand_k) - np.median(good_k))\n",
    "            u_gr, p_gr = mannwhitneyu(good_k, rand_k, alternative=\"less\")  # good < rand\n",
    "            u_gb, p_gb = mannwhitneyu(good_k, bad_k, alternative=\"less\")   # good < bad\n",
    "\n",
    "            uplift_rows.append({\"K\": kk, \"d_median_rand_minus_good\": dmed_rg,\n",
    "                                \"p_good_less_rand\": float(p_gr), \"p_good_less_bad\": float(p_gb)})\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(6,5))\n",
    "            ax.boxplot([good_k, rand_k, bad_k], labels=[\"Top-K GOOD\",\"Random-K\",\"Top-K BAD\"], showmeans=True)\n",
    "            ax.set_title(f\"Top-{kk} uplift\\nΔmedian(random−good)={dmed_rg:.3f};  p: good<rand {p_gr:.2g}, good<bad {p_gb:.2g}\")\n",
    "            ax.set_ylabel(\"Oracle loss (lower is better)\")\n",
    "            fig.tight_layout()\n",
    "            outp = out_dir / f\"box_uplift_K{kk}.png\"\n",
    "            fig.savefig(outp, dpi=170, bbox_inches=\"tight\")\n",
    "            if show:\n",
    "                try:\n",
    "                    from IPython.display import display\n",
    "                    display(fig)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if return_figs:\n",
    "                figs[\"boxplots\"].append(fig)   # <-- CORRECT KEY\n",
    "            else:\n",
    "                plt.close(fig)\n",
    "            boxplot_paths.append(str(outp))\n",
    "\n",
    "\n",
    "    # C) GP slices with prior overlay (GOOD and BAD if present)\n",
    "    overlay = {}\n",
    "    if \"GOOD\" in priors: overlay[\"GOOD\"] = priors[\"GOOD\"]\n",
    "    if \"BAD\"  in priors: overlay[\"BAD\"]  = priors[\"BAD\"]\n",
    "\n",
    "    gp_paths, gp_figs = gp_fit_and_slices(schema, X_pool, y_loss, out_dir, overlay, grid_n=200, seed=seed, show=show, return_figs=return_figs)\n",
    "    figs[\"gp_slices\"] = gp_figs\n",
    "\n",
    "    # Summaries\n",
    "    spearman_df = pd.DataFrame(spearman_rows).sort_values(\"spearman_rho\", ascending=False)\n",
    "    uplift_df   = pd.DataFrame(uplift_rows) if len(uplift_rows)>0 else pd.DataFrame()\n",
    "\n",
    "    spearman_df.to_csv(out_dir / \"prior_vs_oracle_spearman.csv\", index=False)\n",
    "    uplift_df.to_csv(out_dir / \"topk_uplift_summary.csv\", index=False)\n",
    "\n",
    "    with open(out_dir / \"artifacts.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"scatter\": scatter_paths,\n",
    "            \"uplift_boxplots\": boxplot_paths,\n",
    "            \"gp_slices\": gp_paths\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(\"[OK] Saved artifacts to\", out_dir)\n",
    "    print(\" - scatter:\", len(scatter_paths), \"figs\")\n",
    "    print(\" - uplift boxplots:\", len(boxplot_paths), \"figs\")\n",
    "    print(\" - GP slices:\", len(gp_paths), \"figs\")\n",
    "    print(\" - tables: prior_vs_oracle_spearman.csv, topk_uplift_summary.csv\")\n",
    "\n",
    "    return {\n",
    "        \"scatter\": scatter_paths,\n",
    "        \"uplift_boxplots\": boxplot_paths,\n",
    "        \"gp_slices\": gp_paths,\n",
    "        \"spearman_csv\": str(out_dir / \"prior_vs_oracle_spearman.csv\"),\n",
    "        \"uplift_csv\": str(out_dir / \"topk_uplift_summary.csv\"),\n",
    "        \"figs\": figs if return_figs else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3fb35754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirr\\anaconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\amirr\\anaconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM readout] Success, readout from\n",
      "{'effects': {'QAgNO3(%)': {'effect': 'decrease', 'scale': 0.8, 'confidence': 0.9, 'range_hint': [1.0, 1.0]}, 'Qpva(%)': {'effect': 'nonmonotone-peak', 'scale': 0.7, 'confidence': 0.8, 'range_hint': [1.0, 1.0]}, 'Qtsc(%)': {'effect': 'decrease', 'scale': 0.6, 'confidence': 0.85, 'range_hint': [0.5, 1.0]}, 'Qseed(%)': {'effect': 'increase', 'scale': 0.75, 'confidence': 0.85, 'range_hint': [0.498851653, 1.0]}, 'Qtot(uL/min)': {'effect': 'increase-saturating', 'scale': 0.9, 'confidence': 0.9, 'range_hint': [1.0, 1.0]}}, 'interactions': [{'pair': ['QAgNO3(%)', 'Qseed(%)'], 'type': 'antagonism', 'confidence': 0.9}, {'pair': ['Qtsc(%)', 'QAgNO3(%)'], 'type': 'synergy', 'confidence': 0.8}], 'category_similarity': {}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirr\\AppData\\Local\\Temp\\ipykernel_26984\\2677727837.py:402: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rho, _ = spearmanr(x[idx], y[idx])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirr\\AppData\\Local\\Temp\\ipykernel_26984\\2677727837.py:592: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot([good_k, rand_k, bad_k], labels=[\"Top-K GOOD\",\"Random-K\",\"Top-K BAD\"], showmeans=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirr\\AppData\\Local\\Temp\\ipykernel_26984\\2677727837.py:592: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot([good_k, rand_k, bad_k], labels=[\"Top-K GOOD\",\"Random-K\",\"Top-K BAD\"], showmeans=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirr\\AppData\\Local\\Temp\\ipykernel_26984\\2677727837.py:592: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot([good_k, rand_k, bad_k], labels=[\"Top-K GOOD\",\"Random-K\",\"Top-K BAD\"], showmeans=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved artifacts to priors_preAL\n",
      " - scatter: 4 figs\n",
      " - uplift boxplots: 3 figs\n",
      " - GP slices: 5 figs\n",
      " - tables: prior_vs_oracle_spearman.csv, topk_uplift_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "art = run_prior_validity(\n",
    "    oracle_path=\"agnp_oracle_out/oracle.pkl\",\n",
    "    data_csv=\"AgNP_dataset.csv\",\n",
    "    out_dir=\"priors_preAL\",\n",
    "    n_samples=1500, seed=123, k_list=(10,25,50),\n",
    "    priors_wanted=(\"GOOD\",\"BAD\",\"Heuristic\",\"NoPrior\"),\n",
    "    show=True,          # display inline\n",
    "    return_figs=True    # also return the figure objects\n",
    ")\n",
    "\n",
    "# Access figure objects if you want to tweak or re-save:\n",
    "scatters = art[\"figs\"][\"scatter\"]\n",
    "boxplots = art[\"figs\"][\"boxplots\"]\n",
    "gp_slices = art[\"figs\"][\"gp_slices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirr\\anaconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\amirr\\anaconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#manual check of the process \n",
    "\n",
    "oracle_path=\"agnp_oracle_out/oracle.pkl\"\n",
    "data_csv=\"AgNP_dataset.csv\"\n",
    "out_dir=\"priors_preAL\"\n",
    "n_samples=1500\n",
    "seed=123\n",
    "k_list=(10,25,50)\n",
    "priors_wanted=(\"GOOD\",\"BAD\",\"Heuristic\",\"NoPrior\")\n",
    "show=True\n",
    "return_figs=True    # also return the figure objects\n",
    "\n",
    "scatters = art[\"figs\"][\"scatter\"]\n",
    "boxplots = art[\"figs\"][\"boxplots\"]\n",
    "gp_slices = art[\"figs\"][\"gp_slices\"]\n",
    "\n",
    "from agnp_oracle import AgNPOracle\n",
    "out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "schema, df_all = derive_schema(str(data_csv))\n",
    "features = [v.name for v in schema.continuous]\n",
    "oracle = AgNPOracle(str(oracle_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8250b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM readout] Success, readout from\n",
      "{'effects': {'QAgNO3(%)': {'effect': 'increase', 'scale': 0.8, 'confidence': 0.9, 'range_hint': [1.0, 1.0]}, 'Qpva(%)': {'effect': 'increase', 'scale': 0.7, 'confidence': 0.8, 'range_hint': [1.0, 1.0]}, 'Qtsc(%)': {'effect': 'decrease', 'scale': 0.6, 'confidence': 0.85, 'range_hint': [0.5, 1.0]}, 'Qseed(%)': {'effect': 'decrease', 'scale': 0.75, 'confidence': 0.9, 'range_hint': [0.498851653, 1.0]}, 'Qtot(uL/min)': {'effect': 'increase-saturating', 'scale': 0.7, 'confidence': 0.85, 'range_hint': [1.0, 1.0]}}, 'interactions': [{'pair': ['QAgNO3(%)', 'Qseed(%)'], 'type': 'antagonism', 'confidence': 0.9}, {'pair': ['Qtsc(%)', 'QAgNO3(%)'], 'type': 'synergy', 'confidence': 0.8}], 'category_similarity': {}}\n",
      "[LLM readout] Success, readout from\n",
      "{'effects': {'QAgNO3(%)': {'effect': 'decrease', 'scale': 0.8, 'confidence': 0.9, 'range_hint': [0.0, 0.02]}, 'Qpva(%)': {'effect': 'increase', 'scale': 0.7, 'confidence': 0.85, 'range_hint': [0.0, 0.4]}, 'Qtsc(%)': {'effect': 'flat', 'scale': 0.0, 'confidence': 1.0}, 'Qseed(%)': {'effect': 'increase', 'scale': 0.9, 'confidence': 0.95, 'range_hint': [0.195, 0.5]}, 'Qtot(uL/min)': {'effect': 'flat', 'scale': 0.0, 'confidence': 1.0}}, 'interactions': [], 'category_similarity': {}}\n",
      "GOOD Spearman: SignificanceResult(statistic=np.float64(0.6161698745273604), pvalue=np.float64(1.9722982698523574e-209))\n",
      "BAD  Spearman: SignificanceResult(statistic=np.float64(-0.6094440738105998), pvalue=np.float64(1.0642665860536487e-203))\n"
     ]
    }
   ],
   "source": [
    "# Build priors\n",
    "llm_fn = make_openai_llm_fn()  # None if no key\n",
    "good_readout = llm_readout(schema, GOOD_BULLETS, llm_fn, out_dir / \"llm_good_readout.json\",)\n",
    "bad_readout  = llm_readout(schema, BAD_BULLETS,  llm_fn, out_dir / \"llm_bad_readout.json\")\n",
    "heur_readout = build_heuristic(schema)\n",
    "no_readout   = build_no_prior(schema)\n",
    "all_priors = {\"GOOD\":good_readout, \"BAD\":bad_readout, \"Heuristic\":heur_readout, \"NoPrior\":no_readout}\n",
    "priors = {k: all_priors[k] for k in priors_wanted if k in all_priors}\n",
    "figs = {\"scatter\": [], \"boxplots\": [], \"gp_slices\": []}\n",
    "# Sample pool + evaluate oracle\n",
    "X_pool = sample_pool(schema, int(n_samples), seed=seed)\n",
    "y_loss = np.array(oracle.evaluate_batch(X_pool[features].to_dict(orient=\"records\")), dtype=float)\n",
    "\n",
    "# Prior scores\n",
    "prior_scores = {name: prior_score_df(schema, ro, X_pool) for name, ro in priors.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "40690d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'effects': {'QAgNO3(%)': {'effect': 'decrease',\n",
       "   'scale': 0.8,\n",
       "   'confidence': 0.9,\n",
       "   'range_hint': [1.0, 1.0]},\n",
       "  'Qpva(%)': {'effect': 'nonmonotone-peak',\n",
       "   'scale': 0.7,\n",
       "   'confidence': 0.8,\n",
       "   'range_hint': [1.0, 1.0]},\n",
       "  'Qtsc(%)': {'effect': 'decrease',\n",
       "   'scale': 0.6,\n",
       "   'confidence': 0.85,\n",
       "   'range_hint': [0.5, 1.0]},\n",
       "  'Qseed(%)': {'effect': 'increase',\n",
       "   'scale': 0.75,\n",
       "   'confidence': 0.85,\n",
       "   'range_hint': [0.498851653, 1.0]},\n",
       "  'Qtot(uL/min)': {'effect': 'increase-saturating',\n",
       "   'scale': 0.9,\n",
       "   'confidence': 0.9,\n",
       "   'range_hint': [1.0, 1.0]}},\n",
       " 'interactions': [{'pair': ['QAgNO3(%)', 'Qseed(%)'],\n",
       "   'type': 'antagonism',\n",
       "   'confidence': 0.9},\n",
       "  {'pair': ['Qtsc(%)', 'QAgNO3(%)'], 'type': 'synergy', 'confidence': 0.8}],\n",
       " 'category_similarity': {},\n",
       " 'meta': {'origin': 'LLM',\n",
       "  'bullets': [\"Lower 'loss' corresponds to closer match to the prism target spectrum (cosine-shape + amplitude gate).\",\n",
       "   'Expect lower loss with higher silver nitrate ratio QAgNO3 within feasible range; seeds ratio Qseed should be lower.',\n",
       "   'Lower QTSC tends to improve shape matching when targeting triangular prisms; QPVA moderate-to-high stabilizes.',\n",
       "   'Higher total flow Qtot improves mixing and can reduce loss; diminishing returns at the very top of the range.',\n",
       "   'Anticipate interaction between QAgNO3 and Qseed (antagonism): high nitrate + low seeds helpful; and QTSC with QAgNO3.'],\n",
       "  'schema_fingerprint': [('QAgNO3(%)',\n",
       "    4.53,\n",
       "    42.80981595,\n",
       "    'silver-nitrate ratio'),\n",
       "   ('Qpva(%)',\n",
       "    9.999518096,\n",
       "    40.00101474,\n",
       "    'polyvinyl alcohol ratio (stabilizer)'),\n",
       "   ('Qtsc(%)', 0.5, 30.5, 'trisodium citrate ratio (shape-directing)'),\n",
       "   ('Qseed(%)', 0.498851653, 19.5, 'silver seeds ratio'),\n",
       "   ('Qtot(uL/min)', 200.0, 983.0, 'total flow rate (mixing speed)')],\n",
       "  'llm_model': 'gpt-4o-mini',\n",
       "  'key': 'c4523793fb54',\n",
       "  'timestamp': '2025-09-10 14:35:12'}}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_readout(schema, GOOD_BULLETS, llm_fn, out_dir / \"llm_good_readout.json\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fa174f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'effects': {'QAgNO3(%)': {'effect': 'increase-saturating',\n",
       "   'scale': 0.6,\n",
       "   'confidence': 0.7,\n",
       "   'range_hint': [0.6, 1.0]},\n",
       "  'Qpva(%)': {'effect': 'nonmonotone-peak',\n",
       "   'scale': 0.3,\n",
       "   'confidence': 0.4,\n",
       "   'range_hint': [0.5, 0.9]},\n",
       "  'Qtsc(%)': {'effect': 'decrease', 'scale': 0.4, 'confidence': 0.6},\n",
       "  'Qseed(%)': {'effect': 'decrease', 'scale': 0.6, 'confidence': 0.7},\n",
       "  'Qtot(uL/min)': {'effect': 'increase-saturating',\n",
       "   'scale': 0.35,\n",
       "   'confidence': 0.5,\n",
       "   'range_hint': [0.6, 1.0]}},\n",
       " 'interactions': [{'pair': ['QAgNO3(%)', 'Qseed(%)'],\n",
       "   'type': 'antagonism',\n",
       "   'confidence': 0.6},\n",
       "  {'pair': ['QAgNO3(%)', 'Qtsc(%)'], 'type': 'antagonism', 'confidence': 0.4}],\n",
       " 'category_similarity': {}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors['Heuristic']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
